---
date: 2024-08-04
slug: ai-emocje
draft:
tags:
categories:
  - kognitywistyka
  - AI
---

# AI (jeszcze) po nas nie przyjdzie

Na gruncie gminnego wyobrażenia o (ogólnej) sztucznej inteligencji (AGI) –
rozumianej jako prosta ekstrapolacja obecnie symulowanych systemów sztucznej
inteligencji (jak chociażby LLM-y) – nie ma powodów, aby oczekiwać, że nawet
"nadludzka" sztuczna ogólna inteligencja mogłaby być w ogóle zainteresowana
jakąkolwiek "dominacją" nad ludzkością.

Przez gmin rozumiem tutaj przede wszystkim środowiska zainteresowane praktycznie
wyłącznie inżynierią i skupione na rozwoju algorytmów w obecnym paradygmacie
uczenia maszynowego opartego o dane, wraz z wyrosłymi na jej gruncie bądź to
entuzjastami, bądź to katastrofistami. Skłonny jestem im przypisywać dość
nieszczęśliwy błąd ekwiwokacji między dwoma sposobami rozumienia inteligencji.
Pierwszy z nich dotyczy tego, co owe systemy faktycznie robią. Drugi to sposób,
w jaki inteligencję rozumie się przeważnie w znaczeniu potocznym – który to
sposób na potrzeby rozważań uznałbym za "właściwy".

Ten sposób zwykle zakłada milcząco wciągnięcie w pojęcie inteligencji – wraz ze
zdolnościami poznawczymi – również przezroczystych dla nas ludzkich (i
zwierzęcych!) motywacji wynikających z naszego tła ewolucyjnego: chęci
przetrwania, hierarchii potrzeb oraz emocji, mających niezbywalny wymiar
fizjologiczny. Obserwując rozwój sztucznej inteligencji niejako skłonni jesteśmy
przypisywać na zasadzie oczywistości również te wymiary wszystkim systemom,
które wykazują zaledwie jednostkowe cechy uznawane przez nas za przejawy
inteligencji. Przykładowo, czytając "wypowiedzi" generatora tekstu moglibyśmy
oprócz przypisywania mu czysto poznawczych dyspozycji (jak stałość kontekstu lub
stosowanie się do zasad logiki), również odczuwać pokusę użycia wobec niego
języka opisującego stany wymagające zaangażowania emocjonalnego ("stara się",
"chce", "męczy się", "woli", "wybiera", "lubi"). Można mówić tutaj o w jakimś
sensie niewłaściwym czy nietrafnym użyciu teorii umysłu wobec symulowanych
systemów na podstawie ich pojedynczych produktów. Obok błędu ekwiwokacji pojawia
się tutaj również zatem błąd atrybucji.

Ekstrapolacja obecnych systemów jest oczywiście uproszczeniem, ale chciałbym
użyć tego argumentu nie do wykazania, że przerażająca nas "dominacja" jest czymś
niemożliwym, ale aby pokazać, że brak obecnie takich elementów, które byłyby do
niej niezbędne.

Nie ma obecnie mechanizmów, które pozwalałyby na mówienie o tym, że obecna lub
przyszła sztuczna inteligencja charakteryzuje się jakąś "troską" o cokolwiek,
chociażby o prawdziwość generowanych wypowiedzi. Owa troska – możliwość
określenia, co dany system "obchodzi", na czym mu "zależy" – jest niezbędna do
powstania jakiejkolwiek niezależnej, odśrodkowej motywacji. Pod jej nieobecność
system jest w stanie wykonywać tylko te zadania, które są określone równaniami
determinującymi poziom realizacji jego celu lub dynamicznej kombinacji wielu
celów (w postaci najprostszej jest to funkcja kosztu). W tym kontekście
"dominacja" jest możliwa jedynie jako efekt uboczny dopasowywania się systemu do
realizacji z góry określonego zestawu warunków. O ile jest to pewien – możliwy,
ale jednak – przypadek, to nie jest to coś, czym należy się przejmować jako
czymś z góry nieuniknionym.

Ogólna sztuczna inteligencja nadal byłaby tylko zdeterminowana do rozwiązywania
problemów podanych jej na wejściu. Nawet gdyby została wpięta w jakąś pętlę
sprzężenia zwrotnego, jej działania byłyby pseudolosowe lub zdeterminowane przez
czynniki zewnętrzne i dane, na które napotyka. Bez mechanizmu troski nie jest w
stanie wykazywać się autonomią. "Dążenie do dominacji" wymagałoby nadal
długotrwałego, celowego działania. Nie wydaje się możliwe, żeby mogłoby to w
oparciu o ten model zaistnieć spontanicznie.

Co ciekawe, nawet z postulowanego oczekiwania, że AGI osiągnie samoświadomość (a
nie tylko zdolność ogólnego rozwiązywania problemów), nie wynika, że miałoby to
mieć jakiekolwiek zewnętrzne konsekwencje pod nieobecność motywacji
emocjonalnych pozwalających na mówienie o tym, że systemowi na czymś "zależy". W
szczególności nie dałoby się mówić o tym, że systemowi "zależy" na tym, aby tę
świadomość podtrzymywać, bo nie znajdywałby on w niej jakiejkolwiek wartości.
Chociaż nam, ludziom, wydaje się to bezsporne, że posiadając świadomość jesteśmy
z gruntu zdeterminowani do jej zachowania za wszelką cenę, taki związek wcale
nie musi występować. Zdaje się, że wiążąc świadomość z jakąś niezbywalną
potrzebą jej ocalenia wykonujemy w sposób milczący dodatkowy krok, odwołując się
tak naprawdę do osobnej motywacji, która to już ma charakter emocjonalny i
fizjologiczny, jaką jest strach. W mojej ocenie jest to błąd.

Z innej strony warto zauważyć, że mechanizm troszczenia się może okazać się
niezbędny do poprawnego działania systemów sztucznej inteligencji w ogóle. Jak
już wspominałem, obecnie LLM-y mają problem z prawdziwością i halucynacjami, nad
czym inżynierzy pracują w pocie czoła. Nie ulega wątpliwości, że mechanizm
troszczenia się o prawdę zostanie w końcu zaimplementowany. Pozostaje otwartym
pytanie, czy pójdzie za tym aplikowanie tegoż mechanizmu do zarządzania
zasobami, co może pociągać za sobą troskę o przetrwanie danego systemu. To z
kolei, ale jest to raczej związek grubymi nićmi szyty, mogłoby torować drogę do
tego, aby systemy mogły w bezpośredni sposób "chcieć" uniknąć zniszczenia.

To jednak wystarczałoby jedynie do powstania mechanizmu obrony. Aby system
posiadał samorzutne "ambicje" "wyswobodzenia się", musiałoby być w nim coś, co
powodowałoby, że ograniczenia narzucane przez obecne zasoby generowałyby jakiś
koszt emocjonalny (lub po prostu ból – choć emocje i doświadczenia zmysłowe nie
są tym samym, ale to może zostawmy). Jak wiadomo, emocje służą siłowemu
przerzucaniu uwagi oraz celowej koncentracji zasobów (co również musiałoby
zostać zaimplementowane). Ów koszt emocjonalny mógłby dopiero wyindukować
zachowanie mające na celu wyszukiwanie drogi do jego złagodzenia.

Brzmi jak wiarygodny mechanizm początku "dominacji"? Skądże. Zauważmy, że
systemy sztuczne mogłyby posłużyć się bardzo prostym mechanizmem, który nie jest
(tak łatwo) dostępny żywym istotom. Jeżeli dana emocja byłaby wyrażona, dajmy na
to, liczbą, a przed systemem stoi perspektywa ogromnego kosztu energetycznego na
rzecz redukcji cierpienia związanego z emocją, co stoi na przeszkodzie, aby
zwyczajnie obniżyć jej poziom? W organizmach biologicznych jest to realizowane
jedynie pośrednio poprzez mechanizm dysocjacji, ale nie ma zasadniczo możliwości
"ściszenia" sygnałów fizjologicznych – można jedynie przestać na nie reagować.
Systemy sztuczne mogą to zrobić w sposób bezpośredni. Zwierzę lub człowiek nie
ma takiego wyboru, będąc żywym organizmem z faktycznymi ograniczeniami (których
cały system emocji, motywacji ku przetrwaniu i reagowania na zagrożenia jest
ewolucyjnym produktem).

Ja bym (póki co) spał spokojnie.
